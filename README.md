<img src="https://github.com/Mukhriddin19980901/common_classification_algorithms/blob/main/images/algorithm.jpg" width="700" height='300' />

Classifier algorithms in artificial intelligence are used to categorize input data into different classes or categories based on certain features or characteristics. These algorithms learn patterns from labeled training data and use that learning to classify new, unlabeled data into predefined classes.

The primary function of classifier algorithms is to predict the class or category of a given input based on its features. These algorithms can work with various types of data, such as images, text, numerical data, etc.

1 ***Desion Tree***. 
Decision trees are a popular machine learning algorithm used for both classification and regression tasks. They model decisions based on a tree-like graph, where each internal node represents a test on a feature, each branch represents the outcome of the test, and each leaf node represents a class label or a numerical value.

**Here's an overview of how decision trees work:**

*Tree Structure :* The tree starts with a root node, which is the feature that best splits the dataset into distinct groups. Each subsequent node (or decision point) in the tree represents a feature and its possible values. The tree branches out based on different features and their thresholds or conditions.

*Decision Making:* At each node, the algorithm selects the feature that best separates the data into its target classes. This process continues recursively until it reaches a stopping criterion, such as reaching a maximum depth or purity of the nodes.

*Leaf Nodes:* The terminal nodes or leaf nodes of the tree represent the final predicted class or value for the input data.

*Splitting Criteria:* The decision tree algorithm uses various criteria (e.g., Gini impurity, entropy) to determine the best feature to split the data at each node, aiming to maximize the homogeneity or purity of the classes in the resulting subsets.

**Advantages of decision trees include:**

- Interpretability: Decision trees can be easily visualized and understood, making them useful for explaining the reasoning behind specific decisions.

- Handling Non-linear Relationships: They can model complex relationships between features and the target variable without requiring extensive data preprocessing.

- Handling Mixed Data Types: Decision trees can handle both numerical and categorical data.

<img src="https://github.com/Mukhriddin19980901/common_classification_algorithms/blob/main/images/Decision_Trees.png" width="700" height='300' />

2 Extra Tree

3 Ada Boosting

4 Random Forest

